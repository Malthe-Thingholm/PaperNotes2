# ScalO-RAN: Energy-aware Network Intelligence Scaling in Open RAN
ScalO-RAN: Energy-aware Network Intelligence Scaling in Open RAN [arXiv:2312.05096](https://arxiv.org/abs/2312.05096)

This paper brings a third angle to this trilogy of RAN papers. Where the first approached it from the agent's point of view requiring a neutral host, the second not touching on the distribution-algorithm and instead focusing on the feasibility of the hardware-cloud synergy, this paper approaches it mainly from the vendor's point of view with a guarantee of focusing on low latency whilst minimizing costs in the form of energy consumption.

It does this through again the same type of optimization problem, in this case with around double the amount of constraints. One area that wastes energy is spinning up new instances just to delete them after providing their service once. Instead the services should queue, and then we enter a world where adversarial strategies may be viable.

From my background, the first papers focus on a neutral host seems at a glance to at least provide some guarantees and incentives such that a truthful strategy of engagement seems somewhat optimal. This paper raises the point that the utility for the vendor is lessened by the cost of electricity, whilst the utility for the players (participants other than the vendor, i.e. the ones with services) is strongly correlated with latency though some services are more time-sensitive than others they all want to run in a somewhat timely manner. So this paper gives utility guarantees (the other part that should be provided for a 'game' type of setting). 

Without knowing more about cancellation policy etc. i cannot make a full analysis over possible strategies for players that are non-truthful and utility maximizing, however if any holes exist in 'coorporate' policies then they would over time find such holes and this would be decidedly wasteful in regards to energy consumption and such. A straightforward example would be that a bundle of multiple requests might get priority in the algorithm (as it keeps the service spinning with minimum change), so a highly time-sensitive request (fx. stock-market information for automated trading) might want to bundle their real request with multiple 'dummy' requests (probably just copies) such that they get prioritized, even if the cost for them is on a per-request basis.

This kind of hole could be patched by introducing priority bidding through buying requests that have different latency guarantees (i.e. tier 1 has 0.1s latency, tier 2 1s, tier 3 5s etc.) with a proper pricing structure. This would require additional constraints and probably a decidedly different structure for the schedulers and such, so restructuring to patch holes is obviously non-optimal and thus it would be preferable to come up with some sort of system where optimal bidding strategies are thought of and accepted that they will become the standard before it is put into place.
