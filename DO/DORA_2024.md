# DORA 2024
DORA 2024 [https://services.google.com/fh/files/misc/2024_final_dora_report.pdf](https://services.google.com/fh/files/misc/2024_final_dora_report.pdf)

Running commentary for this one because of length.

Graph on page 18 (and further graphs) have a confidence interval, I am guessing that it is meant as 'we have sampled this set of DevOps practitioners, how does this describe the general population of DevOps practitioners'. But the population that is sampled from is not clearly defined. The way that they angle their findings also clearly aligns with their interest of furthering AI adoption.

Again, a lot of talk about how AI is used, is this in relation to DevOps specifically? CI/CD stuff (i.e. most Cloud-Native and DevOps stuff) aligns heavily with templating, for which AI could actually be useful through it inferring templates dynamically, where the end user is mostly responsible for the fine adjustments (for which the AI should also give it a guess). Having the angle of 'give up control of the entire pipeline and everything else to AI' instead of 'templates at different levels would be useful given the repetitive nature of tasks where only minimal adjustments are needed' is something else. Insane levels of fearmongering in some paragraphs.

Next part, again roles that rely on templating that interact most positively with AI usage, but also security professionals. Hopefully security isn't using it for actual checks etc.  

"these gains do not translate to improved software delivery performance. In fact, AI adoption appears detrimental in this area, while its effect on product performance remains negligible, Despite these challenges, AI adoption is linked to improved team and organizational performance." Truly takes more than two braincells to understand why...

This is some next level glazing of AI.

Wrt. toilsome work vs valuable work. Say for DevOps, then what the AI (I'm gonna guess that they mean LLM based AI and act accordingly) is doing is coming up with the template, where the human needs to come up with all the parameters that need adjustment. Here the valuable part would be knowing what kind of 'template-like' code would need to be synthesized (what the AI takes over), and the toilsome work is ensuring that all the proper minor adjustments are made. Which although not necessarily true, feel like a slug could do if it knew what the task was. And if the AI makes a fundamental error (wrong template type fx.) you've gone from being in the seat of power and knowledge, to first questioning your own expertise, to being frustrated with being led down a fundamentally wrong path (where measurably ensuring that the AI is wrong right now feels SUPER toilsome).

I actually do agree with AI generated code (if it's working) generally being capable of raising the code quality level, as it's very formulaic. But ANY increase in documentation is a complete red flag. AI (LLM) cannot know 'why' something is done nor 'how' it's supposed to work. Any documentation is merely a good guess from it of what kind of 'human-speech sentences' are associated with the patchwork of code it pieced together. You might as well have written 'copy this code segment into ChatGPT and ask for what it does' for all the good that does for documentation.

Tagging on to the documentation part with a positive, copying ACTUAL documentation into an LLM and asking it for the relevant segments could be useful if the reader isn't capable of dedicating the necessary time to read and the writer didn't come up with sensible names that would make the relevant sections apparent with a simple 'CTRL + F'.

Having AI drive an awl through my ocular socket would help me read this. "High-performing teams and organizations use AI, but products donâ€™t seem to benefit."

Wtf is the abomination of a graph on page 44. (figure 12) At least the AI section is over.

Figure 14 is super interesting. Initial phase of improved efficiency, it stalls out and needs updates to be 'current', and then it enters longterm maintenance.

The platform engineering section was fine. Inspired the thought that maybe (when using internal platforms in large teams) unit tests should be written by others and obfuscated for the one responsible for the implementation, to better ensure not just 'developing for tests'.

High levels of documentation helping product quality in user-centric projects is a surprising insight but makes sense.

In general, the section about documentation is insightful. Them saying that AI inclusion serves as a north star for the company and thus ensures stable priorities whilst demolishing the quality of work being performed makes the overall paper seem slightly indecisive, but is an insightful comment.

A lot of filler, paper is very centred on non-devs it seems.
